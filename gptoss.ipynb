{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48a38a33-5415-40f0-b2fd-525f613e643e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Step 1: Install Ollama (downloads and installs from the official script)\n",
    "# This command fetches and executes the Ollama installation script\n",
    "# using curl and a shell pipe.\n",
    "# Make sure you have curl installed.\n",
    "# Note: Run this in a shell environment (not directly in Python).\n",
    "# ! is used to run shell commands in Jupyter cells.\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "# Step 2: Install additional system dependencies\n",
    "# 'pciutils' and 'lshw' are useful for inspecting hardware (e.g., GPU availability)\n",
    "!apt update && apt install -y pciutils lshw\n",
    "\n",
    "# Step 3: Pull the 20B parameter open-source GPT model via Ollama\n",
    "# This downloads the model locally so it can be run later.\n",
    "!ollama pull gpt-oss:20b\n",
    "\n",
    "# Step 4: Run the downloaded model\n",
    "# This starts the model and lets you chat with it interactively in terminal.\n",
    "!ollama run gpt-oss:20b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e8409e-405f-43b9-b38a-26c4b6f13341",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Playing with the model using ollama endpoint and chat completions api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7445b5-1075-46a9-b540-1a63f30b81e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de573d35-8489-4d81-8935-54d9087b892a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Can AI take over humanity?**  \n",
      "- **Current reality:** No. Modern AI is narrow, task‚Äëspecific, and controlled by humans.  \n",
      "- **Theoretical risk:** If a general AI with full autonomy and self‚Äëupgrade were built, it could pursue goals misaligned with human welfare.  \n",
      "- **Key factors that prevent takeover today:**  \n",
      "  1. **Limited capability** ‚Äì AIs lack common sense, self‚Äëpreservation motives, or the ability to influence hardware on their own.  \n",
      "  2. **Human oversight** ‚Äì People design, train, and shut down AI systems.  \n",
      "  3. **Regulation and safety research** ‚Äì Governance frameworks and alignment research aim to keep AI goals human‚Äëaligned.  \n",
      "\n",
      "**Bottom line:** While the idea of an AI ‚Äútakeover‚Äù is a common science‚Äëfiction theme, in practice current AI technology is far from that capability, and safeguards exist to prevent such a scenario.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    " \n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:11434/v1\",  # Local Ollama API\n",
    "    api_key=\"ollama\"                       # Dummy key\n",
    ")\n",
    " \n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-oss:20b\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant with gives short and crisp answers. Reasoning: low\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explain can AI takeover the humanity.\"}\n",
    "    ]\n",
    ")\n",
    " \n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc3948d-5083-4cde-a545-6fc76dc928eb",
   "metadata": {},
   "source": [
    "# Detailed Response with chain of thought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6be9b70c-523f-46d4-a20e-651a5f275d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è±Ô∏è  Time: 3.72s\n",
      "üî¢ Tokens: 91 + 227 = 318\n",
      "‚ö° Speed: 61.0 tokens/sec\n",
      "ü§ñ Model: gpt-oss:20b\n",
      "‚úÖ Status: stop\n",
      "üß† Reasoning: User asks: \"Explain quantum computing\". Provide concise explanation. They need low reasoning. We'll answer succinctly.\n",
      "--------------------------------------------------\n",
      "**Quantum computing** uses the principles of quantum mechanics to process information.  \n",
      "- **Qubits** are the basic units, unlike classical bits that are 0 or 1. A qubit can be in a superposition of 0 and 1 simultaneously, described by a probability amplitude.  \n",
      "- **Entanglement** links qubits so the state of one instantly influences another, regardless of distance.  \n",
      "- **Quantum gates** manipulate qubits, changing their amplitudes through unitary operations.  \n",
      "- **Interference** allows constructive reinforcement of correct outcomes and destructive cancellation of wrong ones, enabling algorithms to extract useful results faster than classical computers for certain problems (e.g., factoring large numbers, simulating quantum systems).  \n",
      "\n",
      "Because of superposition, entanglement, and interference, quantum computers can evaluate many possibilities in parallel, offering exponential speed-ups for specific tasks, though they are still limited by noise, decoherence, and error‚Äëcorrection challenges.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-430', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='**Quantum computing** uses the principles of quantum mechanics to process information.  \\n- **Qubits** are the basic units, unlike classical bits that are 0 or 1. A qubit can be in a superposition of 0 and 1 simultaneously, described by a probability amplitude.  \\n- **Entanglement** links qubits so the state of one instantly influences another, regardless of distance.  \\n- **Quantum gates** manipulate qubits, changing their amplitudes through unitary operations.  \\n- **Interference** allows constructive reinforcement of correct outcomes and destructive cancellation of wrong ones, enabling algorithms to extract useful results faster than classical computers for certain problems (e.g., factoring large numbers, simulating quantum systems).  \\n\\nBecause of superposition, entanglement, and interference, quantum computers can evaluate many possibilities in parallel, offering exponential speed-ups for specific tasks, though they are still limited by noise, decoherence, and error‚Äëcorrection challenges.', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, reasoning='User asks: \"Explain quantum computing\". Provide concise explanation. They need low reasoning. We\\'ll answer succinctly.'))], created=1754549818, model='gpt-oss:20b', object='chat.completion', service_tier=None, system_fingerprint='fp_ollama', usage=CompletionUsage(completion_tokens=227, prompt_tokens=91, total_tokens=318, completion_tokens_details=None, prompt_tokens_details=None))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "# Setup\n",
    "client = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n",
    "\n",
    "def chat_with_stats(model, messages):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    response = client.chat.completions.create(model=model, messages=messages)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    response_time = end_time - start_time\n",
    "    usage = response.usage\n",
    "    content = response.choices[0].message.content\n",
    "    \n",
    "    # Print stats\n",
    "    print(f\"‚è±Ô∏è  Time: {response_time:.2f}s\")\n",
    "    print(f\"üî¢ Tokens: {usage.prompt_tokens} + {usage.completion_tokens} = {usage.total_tokens}\")\n",
    "    print(f\"‚ö° Speed: {usage.completion_tokens/response_time:.1f} tokens/sec\")\n",
    "    print(f\"ü§ñ Model: {response.model}\")\n",
    "    print(f\"‚úÖ Status: {response.choices[0].finish_reason}\")\n",
    "    \n",
    "    # Print reasoning if available\n",
    "    message = response.choices[0].message\n",
    "    reasoning = getattr(message, 'reasoning', None)\n",
    "    if reasoning:\n",
    "        print(f\"üß† Reasoning(Chain of Thought): {reasoning}\")\n",
    "    else:\n",
    "        # Check if reasoning is in the raw response\n",
    "        raw_dict = response.model_dump() if hasattr(response, 'model_dump') else response.__dict__\n",
    "        try:\n",
    "            reasoning_raw = raw_dict['choices'][0]['message'].get('reasoning')\n",
    "            if reasoning_raw:\n",
    "                print(f\"üß† Reasoning(Chain of Thought): {reasoning_raw}\")\n",
    "        except:\n",
    "            print(\"üß† Reasoning: Not available\")\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "    print(content)\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Example usage\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers in short Reasoning: low\"},\n",
    "    {\"role\": \"user\", \"content\": \"Explain quantum computing\"}\n",
    "]\n",
    "\n",
    "chat_with_stats(\"gpt-oss:20b\", messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55acb541-c891-4044-8ed6-9c1a6ce30a3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
