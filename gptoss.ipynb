{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48a38a33-5415-40f0-b2fd-525f613e643e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Step 1: Install Ollama (downloads and installs from the official script)\n",
    "# This command fetches and executes the Ollama installation script\n",
    "# using curl and a shell pipe.\n",
    "# Make sure you have curl installed.\n",
    "# Note: Run this in a shell environment (not directly in Python).\n",
    "# ! is used to run shell commands in Jupyter cells.\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "# Step 2: Install additional system dependencies\n",
    "# 'pciutils' and 'lshw' are useful for inspecting hardware (e.g., GPU availability)\n",
    "!apt update && apt install -y pciutils lshw\n",
    "\n",
    "# Step 3: Pull the 20B parameter open-source GPT model via Ollama\n",
    "# This downloads the model locally so it can be run later.\n",
    "!ollama pull gpt-oss:20b\n",
    "\n",
    "# Step 4: Run the downloaded model\n",
    "# This starts the model and lets you chat with it interactively in terminal.\n",
    "!ollama run gpt-oss:20b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e8409e-405f-43b9-b38a-26c4b6f13341",
   "metadata": {},
   "source": [
    "# Playing with the model using ollama endpoint and chat completions api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e7445b5-1075-46a9-b540-1a63f30b81e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de573d35-8489-4d81-8935-54d9087b892a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Short answer:**  \n",
      "No. Current AI systems are narrow and lack the autonomy, motivation, and general intelligence required to ‚Äútake over‚Äù humanity. Even a hypothetical future AGI would need human‚Äëdesigned goals, safeguards, and alignment to act benignly‚Äîan unaligned, super‚Äëintelligent system would be a theoretical risk, not a present reality.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    " \n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:11434/v1\",  # Local Ollama API\n",
    "    api_key=\"ollama\"                       # Dummy key\n",
    ")\n",
    " \n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-oss:20b\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant with gives short and crisp answers. Reasoning: low\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explain can AI takeover the humanity.\"}\n",
    "    ]\n",
    ")\n",
    " \n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc3948d-5083-4cde-a545-6fc76dc928eb",
   "metadata": {},
   "source": [
    "# Detailed Response with chain of thought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6be9b70c-523f-46d4-a20e-651a5f275d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¢ Tokens: 95 + 192 = 287\n",
      "ü§ñ Model: gpt-oss:20b\n",
      "‚úÖ Status: stop\n",
      "üß† Reasoning(Chain of Thought): We need to answer within guidelines. Short reasoning: low. So just answer concisely with short reasoning note. Provide answer: No, can't take over world, technical, ethical constraints.\n",
      "--------------------------------------------------\n",
      "**Answer**: No ‚Äì current AI systems lack the general intelligence, agency, and autonomous control necessary to seize power globally.\n",
      "\n",
      "**Reasoning**:  \n",
      "- AI is narrow, task‚Äëspecific, and requires human oversight for training, deployment, and updates.  \n",
      "- Even advanced models don‚Äôt possess self‚Äëawareness, motives, or the ability to coordinate large‚Äëscale actions without human instructions.  \n",
      "- Safeguards such as regulatory oversight, technical constraints (e.g., alignment, explainability), and legal frameworks are in place to prevent unintended misuse.  \n",
      "- Ethical and societal norms, plus practical limits like hardware dependencies and data requirements, further inhibit a single AI from autonomously ‚Äútaking over‚Äù the world.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-637', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='**Answer**: No ‚Äì current AI systems lack the general intelligence, agency, and autonomous control necessary to seize power globally.\\n\\n**Reasoning**:  \\n- AI is narrow, task‚Äëspecific, and requires human oversight for training, deployment, and updates.  \\n- Even advanced models don‚Äôt possess self‚Äëawareness, motives, or the ability to coordinate large‚Äëscale actions without human instructions.  \\n- Safeguards such as regulatory oversight, technical constraints (e.g., alignment, explainability), and legal frameworks are in place to prevent unintended misuse.  \\n- Ethical and societal norms, plus practical limits like hardware dependencies and data requirements, further inhibit a single AI from autonomously ‚Äútaking over‚Äù the world.', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, reasoning=\"We need to answer within guidelines. Short reasoning: low. So just answer concisely with short reasoning note. Provide answer: No, can't take over world, technical, ethical constraints.\"))], created=1754563727, model='gpt-oss:20b', object='chat.completion', service_tier=None, system_fingerprint='fp_ollama', usage=CompletionUsage(completion_tokens=192, prompt_tokens=95, total_tokens=287, completion_tokens_details=None, prompt_tokens_details=None))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "# Setup\n",
    "client = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n",
    "\n",
    "def chat_with_stats(model, messages):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    response = client.chat.completions.create(model=model, messages=messages)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    response_time = end_time - start_time\n",
    "    usage = response.usage\n",
    "    content = response.choices[0].message.content\n",
    "    \n",
    "    # Print stats\n",
    "    print(f\"üî¢ Tokens: {usage.prompt_tokens} + {usage.completion_tokens} = {usage.total_tokens}\")\n",
    "    print(f\"ü§ñ Model: {response.model}\")\n",
    "    print(f\"‚úÖ Status: {response.choices[0].finish_reason}\")\n",
    "    \n",
    "    # Print reasoning if available\n",
    "    message = response.choices[0].message\n",
    "    reasoning = getattr(message, 'reasoning', None)\n",
    "    if reasoning:\n",
    "        print(f\"üß† Reasoning(Chain of Thought): {reasoning}\")\n",
    "    else:\n",
    "        # Check if reasoning is in the raw response\n",
    "        raw_dict = response.model_dump() if hasattr(response, 'model_dump') else response.__dict__\n",
    "        try:\n",
    "            reasoning_raw = raw_dict['choices'][0]['message'].get('reasoning')\n",
    "            if reasoning_raw:\n",
    "                print(f\"üß† Reasoning(Chain of Thought): {reasoning_raw}\")\n",
    "        except:\n",
    "            print(\"üß† Reasoning: Not available\")\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "    print(content)\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Example usage\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers in short Reasoning: low\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can AI Take over the world?\"}\n",
    "]\n",
    "\n",
    "chat_with_stats(\"gpt-oss:20b\", messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9e4952-650a-456e-b2f0-21210e52b74c",
   "metadata": {},
   "source": [
    "# Funtion Calling with gpt-oss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b41f4a6e-4b6b-4c24-89c3-5bad147517d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model response: ChatCompletionMessage(content='', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_xf584efy', function=Function(arguments='{\"city\":\"Bangalore\"}', name='get_weather'), type='function', index=0)], reasoning='The user asks: \"What\\'s the weather like in Bangalore today?\" We need to use the tool get_weather with city = Bangalore. Return the result.')\n",
      "\n",
      "‚úÖ Final Answer:\n",
      "Here are the weather details for Bangalore today:\n",
      "\n",
      "- **Condition:** Cloudy with light showers  \n",
      "- **Temperature:** 27‚ÄØ¬∞C  \n",
      "\n",
      "Feel free to let me know if you‚Äôd like a forecast for the next few days or any other information!\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Connect to Ollama's local API\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:11434/v1\",  # Ollama API\n",
    "    api_key=\"ollama\"                       # Dummy key\n",
    ")\n",
    "\n",
    "# Define the function (tool) for getting weather\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_weather\",\n",
    "            \"description\": \"Get current weather in a given Indian city\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"city\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\"Mumbai\", \"Delhi\", \"Bangalore\", \"Chennai\", \"Kolkata\"],\n",
    "                        \"description\": \"The name of the city to get weather for\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"city\"]\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "# Initial user message\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What's the weather like in Bangalore today?\"}\n",
    "]\n",
    "\n",
    "# Call the model with the tool definition\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-oss:20b\",\n",
    "    messages=messages,\n",
    "    tools=tools,\n",
    "    tool_choice=\"auto\"\n",
    ")\n",
    "\n",
    "# Extract the model's response\n",
    "message = response.choices[0].message\n",
    "print(\"Model response:\", message)\n",
    "\n",
    "# Step 2: Check if tool_call is requested\n",
    "if message.tool_calls:\n",
    "    for tool_call in message.tool_calls:\n",
    "        tool_name = tool_call.function.name\n",
    "        args = eval(tool_call.function.arguments)\n",
    "\n",
    "        # Dummy weather database\n",
    "        weather_data = {\n",
    "            \"Mumbai\": {\"temp\": \"31¬∞C\", \"condition\": \"Humid and Cloudy\"},\n",
    "            \"Delhi\": {\"temp\": \"36¬∞C\", \"condition\": \"Sunny with haze\"},\n",
    "            \"Bangalore\": {\"temp\": \"27¬∞C\", \"condition\": \"Cloudy with light showers\"},\n",
    "            \"Chennai\": {\"temp\": \"33¬∞C\", \"condition\": \"Hot and Sunny\"},\n",
    "            \"Kolkata\": {\"temp\": \"30¬∞C\", \"condition\": \"Thunderstorms likely\"},\n",
    "        }\n",
    "\n",
    "        # Get the city from tool arguments\n",
    "        city = args.get(\"city\", \"Unknown\")\n",
    "\n",
    "        # Generate response\n",
    "        if city in weather_data:\n",
    "            tool_result = f\"The weather in {city} is {weather_data[city]['condition']} with a temperature of {weather_data[city]['temp']}.\"\n",
    "        else:\n",
    "            tool_result = f\"Sorry, I don't have weather data for {city}.\"\n",
    "\n",
    "        # Feed tool result back into the model for final response\n",
    "        final_response = client.chat.completions.create(\n",
    "            model=\"gpt-oss:20b\",\n",
    "            messages=[\n",
    "                messages[0],  # original user message\n",
    "                message,      # model's tool_call response\n",
    "                {\n",
    "                    \"role\": \"tool\",\n",
    "                    \"tool_call_id\": tool_call.id,\n",
    "                    \"name\": tool_name,\n",
    "                    \"content\": tool_result\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        print(\"\\n‚úÖ Final Answer:\")\n",
    "        print(final_response.choices[0].message.content)\n",
    "else:\n",
    "    print(\"No function call was made by the model.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
